{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(image_size=160, margin=16)\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "import cv2\n",
    "face_cascade = cv2.CascadeClassifier('./cascade.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, SubsetRandomSampler\n",
    "import os\n",
    "from PIL import Image, ImageOps, ImageStat, ImageEnhance\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def brightness( im_file ):\n",
    "   im = Image.fromarray(im_file).convert('L')\n",
    "   stat = ImageStat.Stat(im)\n",
    "   return stat.mean[0]\n",
    "\n",
    "\n",
    "class LFW(Dataset):\n",
    "    def __init__(self, path_to_data, transform=None):\n",
    "        self.transform = transform\n",
    "        data_same_person = []\n",
    "        data_dif_person = []\n",
    "        with open(path_to_data, 'r') as f:\n",
    "            line = f.read()\n",
    "            line = line.split('\\n')\n",
    "            line = [i.split('\\t') for i in line]\n",
    "            [data_same_person.append(i) if len(i) == 3 else data_dif_person.append(i) for i in line]\n",
    "        data_same_person_res = []\n",
    "        for i in data_same_person:\n",
    "            if len(i[1]) == 1:\n",
    "                inp = f'./lfw/{i[0]}/{i[0]}_000{i[1]}.jpg'\n",
    "            elif len(i[1]) ==2:\n",
    "                inp = f'./lfw/{i[0]}/{i[0]}_00{i[1]}.jpg'\n",
    "            else:\n",
    "                inp = f'./lfw/{i[0]}/{i[0]}_0{i[1]}.jpg'\n",
    "            if len(i[2]) == 1:\n",
    "                tar = f'./lfw/{i[0]}/{i[0]}_000{i[2]}.jpg'\n",
    "            elif len(i[2]) ==2:\n",
    "                tar = f'./lfw/{i[0]}/{i[0]}_00{i[2]}.jpg'\n",
    "            else:\n",
    "                tar = f'./lfw/{i[0]}/{i[0]}_0{i[2]}.jpg'\n",
    "            data_same_person_res.append([inp, tar, 1.0])\n",
    "\n",
    "        #data_same_person_res = [[f'./lfw/{i[0]}/{i[0]}_000{i[1]}.jpg' if len(i[1]) == 1 else f'./lfw/{i[0]}/{i[0]}_00{i[1]}.jpg',\n",
    "                                 #f'./lfw/{i[0]}/{i[0]}_000{i[2]}.jpg' if len(i[2]) == 1 else f'./lfw/{i[0]}/{i[0]}_00{i[2]}.jpg',\n",
    "                                 #1.0] for i in data_same_person]\n",
    "        del (data_dif_person[-1])\n",
    "        data_dif_person_res = []\n",
    "        for i in data_dif_person:\n",
    "            if len(i[1]) == 1:\n",
    "                inp = f'./lfw/{i[0]}/{i[0]}_000{i[1]}.jpg'\n",
    "            elif len(i[1]) ==2:\n",
    "                inp = f'./lfw/{i[0]}/{i[0]}_00{i[1]}.jpg'\n",
    "            else:\n",
    "                inp = f'./lfw/{i[0]}/{i[0]}_0{i[1]}.jpg'\n",
    "            if len(i[3]) == 1:\n",
    "                tar = f'./lfw/{i[2]}/{i[2]}_000{i[3]}.jpg'\n",
    "            elif len(i[3]) ==2:\n",
    "                tar = f'./lfw/{i[2]}/{i[2]}_00{i[3]}.jpg'\n",
    "            else:\n",
    "                tar = f'./lfw/{i[2]}/{i[2]}_0{i[3]}.jpg'\n",
    "            data_dif_person_res.append([inp, tar, 0.0])\n",
    "        #data_dif_person_res = [[f'./lfw/{i[0]}/{i[0]}_000{i[1]}.jpg' if len(i[1]) == 1 else f'./lfw/{i[0]}/{i[0]}_00{i[1]}.jpg',\n",
    "        #                        f'./lfw/{i[2]}/{i[2]}_000{i[3]}.jpg' if len(i[3]) == 1 else f'./lfw/{i[2]}/{i[2]}_00{i[3]}.jpg',\n",
    "        #                        0.0] for i in data_dif_person]\n",
    "        self.data = data_same_person_res + data_dif_person_res\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    '''def __getitem__(self, index):\n",
    "        img_input = mtcnn(Image.open(self.data[index][0])).detach().numpy()\n",
    "        img_target = mtcnn(Image.open(self.data[index][1])).detach().numpy()\n",
    "        label = float(self.data[index][2])\n",
    "        return img_input, img_target, label'''\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_inp = cv2.imread(self.data[index][0])\n",
    "        img = img_inp.copy()\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        # Detect faces\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "        for (x, y, w, h) in faces:\n",
    "            faces = img_inp[y:y + h, x:x + w]\n",
    "        img_input = faces\n",
    "        img_input = cv2.cvtColor(img_input, cv2.COLOR_BGR2GRAY)\n",
    "        img_input = controller(img_input, 60)\n",
    "\n",
    "\n",
    "\n",
    "        test = Image.fromarray(img_input)\n",
    "        test = ImageOps.grayscale(test)\n",
    "\n",
    "        stat = ImageStat.Stat(test)\n",
    "        stat = stat.mean[0]\n",
    "        filter = ImageEnhance.Brightness(test)\n",
    "        image = filter.enhance(100/brightness(img_input))\n",
    "        image = np.asarray(image)\n",
    "        image = cv2.merge([image, image, image])\n",
    "        image = np.reshape(image, (3, image.shape[0], image.shape[1]))\n",
    "        image = np.resize(image, (3, 160, 160))\n",
    "\n",
    "\n",
    "        tar = cv2.imread(self.data[index][1])\n",
    "        img = tar.copy()\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "        for (x, y, w, h) in faces:\n",
    "            faces = tar[y:y + h, x:x + w]\n",
    "        img_target = faces\n",
    "        img_target = cv2.cvtColor(img_target, cv2.COLOR_BGR2GRAY)\n",
    "        img_target = cv2.merge([img_target, img_target, img_target])\n",
    "        img_target = np.reshape(img_target, (3, img_target.shape[0], img_target.shape[0]))\n",
    "        img_target = np.resize(img_target, (3, 160, 160))\n",
    "        label = float(self.data[index][2])\n",
    "        return image, img_target, label\n",
    "\n",
    "\n",
    "class LFW_Train(Dataset):\n",
    "    def __init__(self, path_to_data, transform=None):\n",
    "        self.transform = transform\n",
    "        data_same_person = []\n",
    "        data_dif_person = []\n",
    "        with open(path_to_data, 'r') as f:\n",
    "            line = f.read()\n",
    "            line = line.split('\\n')\n",
    "            line = [i.split('\\t') for i in line]\n",
    "            [data_same_person.append(i) if len(i) == 3 else data_dif_person.append(i) for i in line]\n",
    "        res = []\n",
    "        for i in data_same_person:\n",
    "            for j in data_dif_person:\n",
    "                if i[0] in j:\n",
    "                    index = j.index(i[0])\n",
    "                    if index == 0:\n",
    "                        index = 2\n",
    "                    else:\n",
    "                        index = 0\n",
    "                    tmp = i.copy()\n",
    "                    tmp.append(j[index])\n",
    "                    tmp.append(j[index+1])\n",
    "                    res.append(tmp)\n",
    "        data_res = []\n",
    "        for i in res:\n",
    "            if len(i[1]) == 1:\n",
    "                anchor = f'./lfw/{i[0]}/{i[0]}_000{i[1]}.jpg'\n",
    "            elif len(i[1]) == 2:\n",
    "                anchor = f'./lfw/{i[0]}/{i[0]}_00{i[1]}.jpg'\n",
    "            else:\n",
    "                anchor = f'./lfw/{i[0]}/{i[0]}_0{i[1]}.jpg'\n",
    "            if len(i[2]) == 1:\n",
    "                pos = f'./lfw/{i[0]}/{i[0]}_000{i[2]}.jpg'\n",
    "            elif len(i[2]) == 2:\n",
    "                pos = f'./lfw/{i[0]}/{i[0]}_00{i[2]}.jpg'\n",
    "            else:\n",
    "                pos = f'./lfw/{i[0]}/{i[0]}_0{i[2]}.jpg'\n",
    "            if len(i[4]) == 1:\n",
    "                neg = f'./lfw/{i[3]}/{i[3]}_000{i[4]}.jpg'\n",
    "            elif len(i[4]) == 2:\n",
    "                neg = f'./lfw/{i[3]}/{i[3]}_00{i[4]}.jpg'\n",
    "            else:\n",
    "                neg = f'./lfw/{i[3]}/{i[3]}_0{i[4]}.jpg'\n",
    "            data_res.append([anchor, pos, neg])\n",
    "        self.data = data_res\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    '''def __getitem__(self, index):\n",
    "        img_input = mtcnn(Image.open(self.data[index][0])).detach().numpy()\n",
    "        img_target = mtcnn(Image.open(self.data[index][1])).detach().numpy()\n",
    "        label = float(self.data[index][2])\n",
    "        return img_input, img_target, label'''\n",
    "\n",
    "    ''' def __getitem__(self, index):\n",
    "        img_inp = Image.open(self.data[index][0])\n",
    "        img_inp = np.asarray(ImageOps.grayscale(img_inp))\n",
    "        img_inp = cv2.merge([img_inp, img_inp, img_inp])\n",
    "        anchor = mtcnn(img_inp).detach().numpy()\n",
    "\n",
    "        pos = Image.open(self.data[index][1])\n",
    "        pos = np.asarray(ImageOps.grayscale(pos))\n",
    "        pos = cv2.merge([pos, pos, pos])\n",
    "        pos = mtcnn(pos).detach().numpy()\n",
    "\n",
    "        tar = Image.open(self.data[index][2])\n",
    "        tar = np.asarray(ImageOps.grayscale(tar))\n",
    "        tar = cv2.merge([tar, tar, tar])\n",
    "        neg = mtcnn(tar).detach().numpy()\n",
    "\n",
    "        return anchor, pos, neg'''\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "\n",
    "        img_inp = cv2.imread(self.data[index][0])\n",
    "        img = img_inp.copy()\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            faces = img_inp[y:y + h, x:x + w]\n",
    "        anchor = faces\n",
    "        #print(self.data[index][0])\n",
    "        anchor = cv2.cvtColor(anchor, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "        img_inp = cv2.imread(self.data[index][1])\n",
    "        img = img_inp.copy()\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "        for (x, y, w, h) in faces:\n",
    "            faces = img_inp[y:y + h, x:x + w]\n",
    "        pos = faces\n",
    "        #print(self.data[index][1])\n",
    "        pos = cv2.cvtColor(pos, cv2.COLOR_BGR2GRAY)\n",
    "        pos = cv2.merge([pos, pos, pos])\n",
    "        pos = np.reshape(pos, (3, pos.shape[0], pos.shape[0]))\n",
    "\n",
    "        img_inp = cv2.imread(self.data[index][2])\n",
    "        img = img_inp.copy()\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "        for (x, y, w, h) in faces:\n",
    "            faces = img_inp[y:y + h, x:x + w]\n",
    "        neg = faces\n",
    "        #print(self.data[index][2])\n",
    "        neg = cv2.cvtColor(neg, cv2.COLOR_BGR2GRAY)\n",
    "        neg = cv2.merge([neg, neg, neg])\n",
    "        neg = np.reshape(neg, (3, neg.shape[0], neg.shape[0]))\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            anchor = self.transform(image=anchor)['image']\n",
    "            anchor = controller(anchor)\n",
    "            anchor = cv2.merge([anchor, anchor, anchor])\n",
    "            anchor = np.reshape(anchor, (3, anchor.shape[0], anchor.shape[0]))\n",
    "        anchor = np.resize(anchor, (3, 160, 160))\n",
    "        pos = np.resize(pos, (3, 160, 160))\n",
    "        neg = np.resize(neg, (3, 160, 160))\n",
    "        return np.array(anchor, dtype=np.float), np.array(pos, dtype=np.float), np.array(neg, dtype=np.float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import random\n",
    "def controller(img, brightness=40, contrast=127):\n",
    "    p = random.randint(1, 3)\n",
    "    if p != 2:\n",
    "        return img\n",
    "    p = random.randint(1, 5)\n",
    "    if p == 1:\n",
    "        brightness = 10\n",
    "    elif p == 2:\n",
    "        brightness = 20\n",
    "    elif p == 3:\n",
    "        brightness = 40\n",
    "    elif p == 4:\n",
    "        brightness == 60\n",
    "    else:\n",
    "        brightness = 80\n",
    "    brightness = int((brightness - 0) * (255 - (-255)) / (510 - 0) + (-255))\n",
    "    contrast = int((contrast - 0) * (127 - (-127)) / (254 - 0) + (-127))\n",
    "    if brightness != 0:\n",
    "        if brightness > 0:\n",
    "            shadow = brightness\n",
    "            max = 255\n",
    "        else:\n",
    "            shadow = 0\n",
    "            max = 255 + brightness\n",
    "        al_pha = (max - shadow) / 255\n",
    "        ga_mma = shadow\n",
    "        # The function addWeighted calculates\n",
    "        # the weighted sum of two arrays\n",
    "        cal = cv2.addWeighted(img, al_pha,\n",
    "                              img, 0, ga_mma)\n",
    "    else:\n",
    "        cal = img\n",
    "    if contrast != 0:\n",
    "        Alpha = float(131 * (contrast + 127)) / (127 * (131 - contrast))\n",
    "        Gamma = 127 * (1 - Alpha)\n",
    "        # The function addWeighted calculates\n",
    "        # the weighted sum of two arrays\n",
    "        cal = cv2.addWeighted(cal, Alpha,\n",
    "                              cal, 0, Gamma)\n",
    "    return cal"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "train_dataset = LFW_Train(\"./pairsDevTrain.txt\",\n",
    "                        transform=A.Compose([\n",
    "                                A.HorizontalFlip(p=0.5),\n",
    "                                A.Rotate(limit=(0, 10), p=0.5)\n",
    "                            ])\n",
    "                      )\n",
    "test_dataset = LFW(\"./pairsDevTest.txt\"\n",
    "                      )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "resnet = resnet.to(device)\n",
    "batch_size = 32\n",
    "\n",
    "data_size = len(train_dataset)\n",
    "validation_fraction = .2\n",
    "\n",
    "\n",
    "val_split = int(np.floor((validation_fraction) * data_size))\n",
    "indices = list(range(data_size))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "val_indices, train_indices = indices[:val_split], indices[val_split:]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                           sampler=train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                         sampler=val_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def compute_f1(model, grt):\n",
    "    model.eval()\n",
    "    for x, y, z in grt:\n",
    "        x = x.to(device=device)\n",
    "        y = y.to(device=device)\n",
    "        z = z.to(device=device)\n",
    "        predictx = model(x.float())\n",
    "        predicty = model(y.float())\n",
    "        predictz = model(z.float())\n",
    "        pred = []\n",
    "        label = []\n",
    "        for i in range(len(predictx)):\n",
    "            pred.append((1+sum(predictx[i]*predicty[i]))/2)\n",
    "            pred.append((1+sum(predictx[i]*predictz[i]))/2)\n",
    "            label.append(1.0)\n",
    "            label.append(0.0)\n",
    "        pred = np.array(list(map(lambda x: int(x > 0.8), pred)))\n",
    "        label = np.array(label)\n",
    "        return f1_score(label, pred)\n",
    "\n",
    "def compute_f1_test(model, grt):\n",
    "    model.eval()\n",
    "    for x, y, label in grt:\n",
    "        x = x.to(device=device)\n",
    "        y = y.to(device=device)\n",
    "        predictx = model(x.float())\n",
    "        predicty = model(y.float())\n",
    "        pred = []\n",
    "        for i in range(len(predictx)):\n",
    "            pred.append((1+sum(predictx[i]*predicty[i]))/2)\n",
    "        pred = np.array(list(map(lambda x: int(x > 0.8), pred)))\n",
    "        return f1_score(label, pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss, optimizer, num_epochs):\n",
    "    loss_history = []\n",
    "    val_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Enter train mode\n",
    "        loss_accum = 0\n",
    "        for i_step, (a, p, n) in enumerate(train_loader):\n",
    "            a_gpu = torch.tensor(a, dtype=torch.float).to(device)\n",
    "            p_gpu = torch.tensor(p, dtype=torch.float).to(device)\n",
    "            n_gpu = torch.tensor(n, dtype=torch.float).to(device)\n",
    "            a_gpu = a_gpu.requires_grad_(True)\n",
    "            p_gpu = p_gpu.requires_grad_(True)\n",
    "            n_gpu = n_gpu.requires_grad_(True)\n",
    "            a_gpu = model(a_gpu)\n",
    "            p_gpu = model(p_gpu)\n",
    "            n_gpu = model(n_gpu)\n",
    "            #a_gpu.requires_grad = True\n",
    "            #p_gpu.requires_grad = True\n",
    "            #n_gpu.requires_grad = True\n",
    "            loss_value = loss(a_gpu, p_gpu, n_gpu)\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            loss_accum += loss_value\n",
    "\n",
    "        ave_loss = loss_accum / i_step\n",
    "        val_f1 = compute_f1(model, val_loader)\n",
    "\n",
    "        loss_history.append(float(ave_loss))\n",
    "        val_history.append(val_f1)\n",
    "\n",
    "        print(\"%f Average loss: %f,, Val f1: %f\" % (epoch, ave_loss, val_f1))\n",
    "\n",
    "    return loss_history, val_history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000000 Average loss: 1.019784,, Val f1: 0.355556\n",
      "1.000000 Average loss: 0.962874,, Val f1: 0.468085\n",
      "2.000000 Average loss: 0.868221,, Val f1: 0.444444\n",
      "3.000000 Average loss: 0.931090,, Val f1: 0.408163\n",
      "4.000000 Average loss: 0.965285,, Val f1: 0.375000\n",
      "5.000000 Average loss: 0.871385,, Val f1: 0.531250\n",
      "6.000000 Average loss: 0.799550,, Val f1: 0.695652\n",
      "7.000000 Average loss: 0.901166,, Val f1: 0.557377\n",
      "8.000000 Average loss: 0.800386,, Val f1: 0.600000\n",
      "9.000000 Average loss: 0.800010,, Val f1: 0.566667\n",
      "10.000000 Average loss: 0.820342,, Val f1: 0.600000\n",
      "11.000000 Average loss: 0.756162,, Val f1: 0.571429\n",
      "12.000000 Average loss: 0.767751,, Val f1: 0.636364\n",
      "13.000000 Average loss: 0.720604,, Val f1: 0.633333\n",
      "14.000000 Average loss: 0.651553,, Val f1: 0.617647\n",
      "15.000000 Average loss: 0.725965,, Val f1: 0.634921\n",
      "16.000000 Average loss: 0.756293,, Val f1: 0.526316\n",
      "17.000000 Average loss: 0.836907,, Val f1: 0.533333\n",
      "18.000000 Average loss: 0.722676,, Val f1: 0.615385\n",
      "19.000000 Average loss: 0.628684,, Val f1: 0.433333\n",
      "20.000000 Average loss: 0.647707,, Val f1: 0.576271\n",
      "21.000000 Average loss: 0.592582,, Val f1: 0.482759\n",
      "22.000000 Average loss: 0.665590,, Val f1: 0.666667\n",
      "23.000000 Average loss: 0.650042,, Val f1: 0.647059\n",
      "24.000000 Average loss: 0.589210,, Val f1: 0.688525\n",
      "25.000000 Average loss: 0.596822,, Val f1: 0.516129\n",
      "26.000000 Average loss: 0.567914,, Val f1: 0.709677\n",
      "27.000000 Average loss: 0.637923,, Val f1: 0.459016\n",
      "28.000000 Average loss: 0.635870,, Val f1: 0.523077\n",
      "29.000000 Average loss: 0.631559,, Val f1: 0.642857\n",
      "30.000000 Average loss: 0.585411,, Val f1: 0.600000\n",
      "31.000000 Average loss: 0.555654,, Val f1: 0.571429\n",
      "32.000000 Average loss: 0.563618,, Val f1: 0.516129\n",
      "33.000000 Average loss: 0.567879,, Val f1: 0.735294\n",
      "34.000000 Average loss: 0.556149,, Val f1: 0.677966\n",
      "35.000000 Average loss: 0.580011,, Val f1: 0.517241\n",
      "36.000000 Average loss: 0.576919,, Val f1: 0.531250\n",
      "37.000000 Average loss: 0.492762,, Val f1: 0.551724\n",
      "38.000000 Average loss: 0.482503,, Val f1: 0.625000\n",
      "39.000000 Average loss: 0.462339,, Val f1: 0.625000\n",
      "40.000000 Average loss: 0.489106,, Val f1: 0.592593\n",
      "41.000000 Average loss: 0.474568,, Val f1: 0.631579\n",
      "42.000000 Average loss: 0.418526,, Val f1: 0.646154\n",
      "43.000000 Average loss: 0.407985,, Val f1: 0.561404\n",
      "44.000000 Average loss: 0.456453,, Val f1: 0.687500\n",
      "45.000000 Average loss: 0.463156,, Val f1: 0.666667\n",
      "46.000000 Average loss: 0.471824,, Val f1: 0.750000\n",
      "47.000000 Average loss: 0.434519,, Val f1: 0.701754\n",
      "48.000000 Average loss: 0.407941,, Val f1: 0.571429\n",
      "49.000000 Average loss: 0.361019,, Val f1: 0.625000\n",
      "50.000000 Average loss: 0.397369,, Val f1: 0.518519\n",
      "51.000000 Average loss: 0.418724,, Val f1: 0.646154\n",
      "52.000000 Average loss: 0.414414,, Val f1: 0.677419\n",
      "53.000000 Average loss: 0.373029,, Val f1: 0.571429\n",
      "54.000000 Average loss: 0.408698,, Val f1: 0.593750\n",
      "55.000000 Average loss: 0.342721,, Val f1: 0.490566\n",
      "56.000000 Average loss: 0.365627,, Val f1: 0.634921\n",
      "57.000000 Average loss: 0.310481,, Val f1: 0.626866\n",
      "58.000000 Average loss: 0.349818,, Val f1: 0.656716\n",
      "59.000000 Average loss: 0.303374,, Val f1: 0.590164\n",
      "60.000000 Average loss: 0.378729,, Val f1: 0.646154\n",
      "61.000000 Average loss: 0.377730,, Val f1: 0.533333\n",
      "62.000000 Average loss: 0.331942,, Val f1: 0.622951\n",
      "63.000000 Average loss: 0.321935,, Val f1: 0.566667\n",
      "64.000000 Average loss: 0.352798,, Val f1: 0.545455\n",
      "65.000000 Average loss: 0.290272,, Val f1: 0.634921\n",
      "66.000000 Average loss: 0.329231,, Val f1: 0.603175\n",
      "67.000000 Average loss: 0.346419,, Val f1: 0.634921\n",
      "68.000000 Average loss: 0.295374,, Val f1: 0.645161\n",
      "69.000000 Average loss: 0.351015,, Val f1: 0.709677\n",
      "70.000000 Average loss: 0.304625,, Val f1: 0.666667\n",
      "71.000000 Average loss: 0.309656,, Val f1: 0.542373\n",
      "72.000000 Average loss: 0.258797,, Val f1: 0.696970\n",
      "73.000000 Average loss: 0.248439,, Val f1: 0.542373\n",
      "74.000000 Average loss: 0.301303,, Val f1: 0.490566\n",
      "75.000000 Average loss: 0.311118,, Val f1: 0.612903\n",
      "76.000000 Average loss: 0.394372,, Val f1: 0.610169\n",
      "77.000000 Average loss: 0.393145,, Val f1: 0.516129\n",
      "78.000000 Average loss: 0.340918,, Val f1: 0.633333\n",
      "79.000000 Average loss: 0.288111,, Val f1: 0.483871\n",
      "80.000000 Average loss: 0.281617,, Val f1: 0.634921\n",
      "81.000000 Average loss: 0.268382,, Val f1: 0.622951\n",
      "82.000000 Average loss: 0.295955,, Val f1: 0.474576\n",
      "83.000000 Average loss: 0.223555,, Val f1: 0.561404\n",
      "84.000000 Average loss: 0.232186,, Val f1: 0.593750\n",
      "85.000000 Average loss: 0.218182,, Val f1: 0.596491\n",
      "86.000000 Average loss: 0.239105,, Val f1: 0.721311\n",
      "87.000000 Average loss: 0.258362,, Val f1: 0.584615\n",
      "88.000000 Average loss: 0.249609,, Val f1: 0.549020\n",
      "89.000000 Average loss: 0.237772,, Val f1: 0.549020\n",
      "90.000000 Average loss: 0.278567,, Val f1: 0.620690\n",
      "91.000000 Average loss: 0.233746,, Val f1: 0.500000\n",
      "92.000000 Average loss: 0.215170,, Val f1: 0.545455\n",
      "93.000000 Average loss: 0.219432,, Val f1: 0.626866\n",
      "94.000000 Average loss: 0.229381,, Val f1: 0.535714\n",
      "95.000000 Average loss: 0.309448,, Val f1: 0.440000\n",
      "96.000000 Average loss: 0.348458,, Val f1: 0.590164\n",
      "97.000000 Average loss: 0.253613,, Val f1: 0.562500\n",
      "98.000000 Average loss: 0.307291,, Val f1: 0.451613\n",
      "99.000000 Average loss: 0.293443,, Val f1: 0.590164\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.6938775510204082"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "params_old = list([param for name, param in resnet.named_parameters() if not name.startswith('logits.')])\n",
    "params_new = list([param for name, param in resnet.named_parameters() if name.startswith('logits.')])\n",
    "\n",
    "parameters = [\n",
    "    {'params': params_old, 'lr': 0.0001, 'momentum': 0.9},\n",
    "    {'params': params_new, 'lr': 0.001, 'momentum': 0.9}\n",
    "]\n",
    "optimizer = optim.Adam(resnet.parameters(), lr=0.001)\n",
    "loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "train_model(resnet, train_loader, val_loader, loss, optimizer, 100)\n",
    "compute_f1_test(resnet, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(resnet.state_dict(), 'm2.w')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('pairs_labled.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5254237288135594\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "resnet.load_state_dict(torch.load('./m1.w'))\n",
    "resnet.eval()\n",
    "resnet.to('cpu')\n",
    "x = []\n",
    "y = []\n",
    "label = []\n",
    "predictx = []\n",
    "predicty = []\n",
    "for i in data:\n",
    "    image = cv2.merge([i[0], i[0], i[0]])\n",
    "    image = np.reshape(image, (3, image.shape[0], image.shape[1]))\n",
    "    image = np.resize(image, (3, 160, 160))\n",
    "    x = torch.Tensor(np.array([image])).to('cpu')\n",
    "    predictx.append(resnet(x.float()).detach().numpy()[0])\n",
    "    target = cv2.merge([i[1], i[1], i[1]])\n",
    "    target = np.reshape(target, (3, target.shape[0], target.shape[1]))\n",
    "    target = np.resize(target, (3, 160, 160))\n",
    "    y = torch.Tensor(np.array([target])).to('cpu')\n",
    "    predicty.append(resnet(y.float()).detach().numpy()[0])\n",
    "\n",
    "    label.append(i[2])\n",
    "pred = []\n",
    "for i in range(len(predictx)):\n",
    "    pred.append((1+sum(predictx[i]*predicty[i]))/2)\n",
    "\n",
    "pred = np.array(list(map(lambda x: int(x > 0.8), pred)))\n",
    "print(f1_score(label, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
