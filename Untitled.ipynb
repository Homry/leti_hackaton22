{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(image_size=160, margin=16)\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, SubsetRandomSampler\n",
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LFW(Dataset):\n",
    "    def __init__(self, path_to_data, transform=None):\n",
    "        self.transform = transform\n",
    "        data_same_person = []\n",
    "        data_dif_person = []\n",
    "        with open(path_to_data, 'r') as f:\n",
    "            line = f.read()\n",
    "            line = line.split('\\n')\n",
    "            line = [i.split('\\t') for i in line]\n",
    "            [data_same_person.append(i) if len(i) == 3 else data_dif_person.append(i) for i in line]\n",
    "        data_same_person_res = []\n",
    "        for i in data_same_person:\n",
    "            if len(i[1]) == 1:\n",
    "                inp = f'./lfw/{i[0]}/{i[0]}_000{i[1]}.jpg'\n",
    "            elif len(i[1]) ==2:\n",
    "                inp = f'./lfw/{i[0]}/{i[0]}_00{i[1]}.jpg'\n",
    "            else:\n",
    "                inp = f'./lfw/{i[0]}/{i[0]}_0{i[1]}.jpg'\n",
    "            if len(i[2]) == 1:\n",
    "                tar = f'./lfw/{i[0]}/{i[0]}_000{i[2]}.jpg'\n",
    "            elif len(i[2]) ==2:\n",
    "                tar = f'./lfw/{i[0]}/{i[0]}_00{i[2]}.jpg'\n",
    "            else:\n",
    "                tar = f'./lfw/{i[0]}/{i[0]}_0{i[2]}.jpg'\n",
    "            data_same_person_res.append([inp, tar, 1.0])\n",
    "\n",
    "        #data_same_person_res = [[f'./lfw/{i[0]}/{i[0]}_000{i[1]}.jpg' if len(i[1]) == 1 else f'./lfw/{i[0]}/{i[0]}_00{i[1]}.jpg',\n",
    "                                 #f'./lfw/{i[0]}/{i[0]}_000{i[2]}.jpg' if len(i[2]) == 1 else f'./lfw/{i[0]}/{i[0]}_00{i[2]}.jpg',\n",
    "                                 #1.0] for i in data_same_person]\n",
    "        del (data_dif_person[-1])\n",
    "        data_dif_person_res = []\n",
    "        for i in data_dif_person:\n",
    "            if len(i[1]) == 1:\n",
    "                inp = f'./lfw/{i[0]}/{i[0]}_000{i[1]}.jpg'\n",
    "            elif len(i[1]) ==2:\n",
    "                inp = f'./lfw/{i[0]}/{i[0]}_00{i[1]}.jpg'\n",
    "            else:\n",
    "                inp = f'./lfw/{i[0]}/{i[0]}_0{i[1]}.jpg'\n",
    "            if len(i[3]) == 1:\n",
    "                tar = f'./lfw/{i[2]}/{i[2]}_000{i[3]}.jpg'\n",
    "            elif len(i[3]) ==2:\n",
    "                tar = f'./lfw/{i[2]}/{i[2]}_00{i[3]}.jpg'\n",
    "            else:\n",
    "                tar = f'./lfw/{i[2]}/{i[2]}_0{i[3]}.jpg'\n",
    "            data_dif_person_res.append([inp, tar, 0.0])\n",
    "        #data_dif_person_res = [[f'./lfw/{i[0]}/{i[0]}_000{i[1]}.jpg' if len(i[1]) == 1 else f'./lfw/{i[0]}/{i[0]}_00{i[1]}.jpg',\n",
    "        #                        f'./lfw/{i[2]}/{i[2]}_000{i[3]}.jpg' if len(i[3]) == 1 else f'./lfw/{i[2]}/{i[2]}_00{i[3]}.jpg',\n",
    "        #                        0.0] for i in data_dif_person]\n",
    "        self.data = data_same_person_res + data_dif_person_res\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    '''def __getitem__(self, index):\n",
    "        img_input = mtcnn(Image.open(self.data[index][0])).detach().numpy()\n",
    "        img_target = mtcnn(Image.open(self.data[index][1])).detach().numpy()\n",
    "        label = float(self.data[index][2])\n",
    "        return img_input, img_target, label'''\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_inp = Image.open(self.data[index][0])\n",
    "        img_input = mtcnn(img_inp).detach().numpy()\n",
    "\n",
    "        tar = Image.open(self.data[index][1])\n",
    "        img_target = mtcnn(tar).detach().numpy()\n",
    "        img_target = np.reshape(img)\n",
    "        label = float(self.data[index][2])\n",
    "        return img_input, img_target, label\n",
    "\n",
    "\n",
    "class LFW_Train(Dataset):\n",
    "    def __init__(self, path_to_data, transform=None):\n",
    "        self.transform = transform\n",
    "        data_same_person = []\n",
    "        data_dif_person = []\n",
    "        with open(path_to_data, 'r') as f:\n",
    "            line = f.read()\n",
    "            line = line.split('\\n')\n",
    "            line = [i.split('\\t') for i in line]\n",
    "            [data_same_person.append(i) if len(i) == 3 else data_dif_person.append(i) for i in line]\n",
    "        res = []\n",
    "        for i in data_same_person:\n",
    "            for j in data_dif_person:\n",
    "                if i[0] in j:\n",
    "                    index = j.index(i[0])\n",
    "                    if index == 0:\n",
    "                        index = 2\n",
    "                    else:\n",
    "                        index = 0\n",
    "                    tmp = i.copy()\n",
    "                    tmp.append(j[index])\n",
    "                    tmp.append(j[index+1])\n",
    "                    res.append(tmp)\n",
    "        data_res = []\n",
    "        for i in res:\n",
    "            if len(i[1]) == 1:\n",
    "                anchor = f'./lfw/{i[0]}/{i[0]}_000{i[1]}.jpg'\n",
    "            elif len(i[1]) == 2:\n",
    "                anchor = f'./lfw/{i[0]}/{i[0]}_00{i[1]}.jpg'\n",
    "            else:\n",
    "                anchor = f'./lfw/{i[0]}/{i[0]}_0{i[1]}.jpg'\n",
    "            if len(i[2]) == 1:\n",
    "                pos = f'./lfw/{i[0]}/{i[0]}_000{i[2]}.jpg'\n",
    "            elif len(i[2]) == 2:\n",
    "                pos = f'./lfw/{i[0]}/{i[0]}_00{i[2]}.jpg'\n",
    "            else:\n",
    "                pos = f'./lfw/{i[0]}/{i[0]}_0{i[2]}.jpg'\n",
    "            if len(i[4]) == 1:\n",
    "                neg = f'./lfw/{i[3]}/{i[3]}_000{i[4]}.jpg'\n",
    "            elif len(i[4]) == 2:\n",
    "                neg = f'./lfw/{i[3]}/{i[3]}_00{i[4]}.jpg'\n",
    "            else:\n",
    "                neg = f'./lfw/{i[3]}/{i[3]}_0{i[4]}.jpg'\n",
    "            data_res.append([anchor, pos, neg])\n",
    "        self.data = data_res\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    '''def __getitem__(self, index):\n",
    "        img_input = mtcnn(Image.open(self.data[index][0])).detach().numpy()\n",
    "        img_target = mtcnn(Image.open(self.data[index][1])).detach().numpy()\n",
    "        label = float(self.data[index][2])\n",
    "        return img_input, img_target, label'''\n",
    "\n",
    "    ''' def __getitem__(self, index):\n",
    "        img_inp = Image.open(self.data[index][0])\n",
    "        img_inp = np.asarray(ImageOps.grayscale(img_inp))\n",
    "        img_inp = cv2.merge([img_inp, img_inp, img_inp])\n",
    "        anchor = mtcnn(img_inp).detach().numpy()\n",
    "\n",
    "        pos = Image.open(self.data[index][1])\n",
    "        pos = np.asarray(ImageOps.grayscale(pos))\n",
    "        pos = cv2.merge([pos, pos, pos])\n",
    "        pos = mtcnn(pos).detach().numpy()\n",
    "\n",
    "        tar = Image.open(self.data[index][2])\n",
    "        tar = np.asarray(ImageOps.grayscale(tar))\n",
    "        tar = cv2.merge([tar, tar, tar])\n",
    "        neg = mtcnn(tar).detach().numpy()\n",
    "\n",
    "        return anchor, pos, neg'''\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_inp = Image.open(self.data[index][0])\n",
    "        anchor = mtcnn(img_inp).detach().numpy()\n",
    "        anchor = np.reshape(anchor, (anchor.shape[1], anchor.shape[2], 3))\n",
    "\n",
    "        anchor = cv2.cvtColor(anchor, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "        pos = Image.open(self.data[index][1])\n",
    "        pos = mtcnn(pos).detach().numpy()\n",
    "        pos = np.reshape(pos, (pos.shape[1], pos.shape[2], 3))\n",
    "\n",
    "        pos = cv2.cvtColor(pos, cv2.COLOR_BGR2GRAY)\n",
    "        pos = cv2.merge([pos, pos, pos])\n",
    "        pos = np.reshape(pos, (3, pos.shape[0], pos.shape[0]))\n",
    "\n",
    "        neg = Image.open(self.data[index][2])\n",
    "        neg = mtcnn(neg).detach().numpy()\n",
    "        neg = np.reshape(neg, (neg.shape[1], neg.shape[2], 3))\n",
    "\n",
    "        neg = cv2.cvtColor(neg, cv2.COLOR_BGR2GRAY)\n",
    "        neg = cv2.merge([neg, neg, neg])\n",
    "        neg = np.reshape(neg, (3, neg.shape[0], neg.shape[0]))\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            anchor = self.transform(image=anchor)['image']\n",
    "            anchor = controller(anchor)\n",
    "            anchor = cv2.merge([anchor, anchor, anchor])\n",
    "            anchor = np.reshape(anchor, (3, anchor.shape[0], anchor.shape[0]))\n",
    "\n",
    "        return anchor, pos, neg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import random\n",
    "def controller(img, brightness=40, contrast=127):\n",
    "    p = random.randint(1, 3)\n",
    "    if p == 2:\n",
    "        return img\n",
    "    p = random.randint(1, 3)\n",
    "    if p == 1:\n",
    "        brightness = 40\n",
    "    elif p == 2:\n",
    "        brightness = 60\n",
    "    else:\n",
    "        brightness = 80\n",
    "    brightness = int((brightness - 0) * (255 - (-255)) / (510 - 0) + (-255))\n",
    "    contrast = int((contrast - 0) * (127 - (-127)) / (254 - 0) + (-127))\n",
    "    if brightness != 0:\n",
    "        if brightness > 0:\n",
    "            shadow = brightness\n",
    "            max = 255\n",
    "        else:\n",
    "            shadow = 0\n",
    "            max = 255 + brightness\n",
    "        al_pha = (max - shadow) / 255\n",
    "        ga_mma = shadow\n",
    "        # The function addWeighted calculates\n",
    "        # the weighted sum of two arrays\n",
    "        cal = cv2.addWeighted(img, al_pha,\n",
    "                              img, 0, ga_mma)\n",
    "    else:\n",
    "        cal = img\n",
    "    if contrast != 0:\n",
    "        Alpha = float(131 * (contrast + 127)) / (127 * (131 - contrast))\n",
    "        Gamma = 127 * (1 - Alpha)\n",
    "        # The function addWeighted calculates\n",
    "        # the weighted sum of two arrays\n",
    "        cal = cv2.addWeighted(cal, Alpha,\n",
    "                              cal, 0, Gamma)\n",
    "    return cal"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "train_dataset = LFW_Train(\"./pairsDevTrain.txt\",\n",
    "                        transform=A.Compose([\n",
    "                                A.HorizontalFlip(p=0.2),\n",
    "                                A.Rotate(limit=(0, 15), p=0.3)\n",
    "                            ])\n",
    "                      )\n",
    "test_dataset = LFW(\"./pairsDevTest.txt\",\n",
    "                       transform=A.Compose([\n",
    "                                A.HorizontalFlip(p=0.2),\n",
    "                                A.Rotate(limit=(0, 15), p=0.3)\n",
    "                            ])\n",
    "                      )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resnet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mresnet\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'resnet' is not defined"
     ]
    }
   ],
   "source": [
    "resnet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "resnet = resnet.to(device)\n",
    "batch_size = 64\n",
    "\n",
    "data_size = len(train_dataset)\n",
    "validation_fraction = .2\n",
    "\n",
    "\n",
    "val_split = int(np.floor((validation_fraction) * data_size))\n",
    "indices = list(range(data_size))\n",
    "np.random.seed(43)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "val_indices, train_indices = indices[:val_split], indices[val_split:]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                           sampler=train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                         sampler=val_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def compute_f1(model, grt):\n",
    "    model.eval()\n",
    "    for x, y, z in grt:\n",
    "        x = x.to(device=device)\n",
    "        y = y.to(device=device)\n",
    "        z = z.to(device=device)\n",
    "        predictx = model(x)\n",
    "        predicty = model(y)\n",
    "        predictz = model(z)\n",
    "        pred = []\n",
    "        label = []\n",
    "        for i in range(len(predictx)):\n",
    "            pred.append((1+sum(predictx[i]*predicty[i]))/2)\n",
    "            pred.append((1+sum(predictx[i]*predictz[i]))/2)\n",
    "            label.append(1.0)\n",
    "            label.append(0.0)\n",
    "        pred = np.array(list(map(lambda x: int(x > 0.8), pred)))\n",
    "        label = np.array(label)\n",
    "        return f1_score(label, pred)\n",
    "\n",
    "def compute_f1_test(model, grt):\n",
    "    model.eval()\n",
    "    for x, y, label in grt:\n",
    "        x = x.to(device=device)\n",
    "        y = y.to(device=device)\n",
    "\n",
    "        predictx = model(x)\n",
    "        predicty = model(y)\n",
    "        pred = []\n",
    "        for i in range(len(predictx)):\n",
    "            pred.append((1+sum(predictx[i]*predicty[i]))/2)\n",
    "        print(pred)\n",
    "        pred = np.array(list(map(lambda x: int(x > 0.8), pred)))\n",
    "        return f1_score(label, pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss, optimizer, num_epochs):\n",
    "    loss_history = []\n",
    "    val_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Enter train mode\n",
    "        loss_accum = 0\n",
    "        for i_step, (a, p, n) in enumerate(train_loader):\n",
    "            a_gpu = torch.Tensor(a).to(device)\n",
    "            p_gpu = torch.Tensor(p).to(device)\n",
    "            n_gpu = torch.Tensor(n).to(device)\n",
    "            a_gpu.requires_grad = True\n",
    "            p_gpu.requires_grad = True\n",
    "            n_gpu.requires_grad = True\n",
    "            loss_value = loss(a_gpu, p_gpu, n_gpu)\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            loss_accum += loss_value\n",
    "\n",
    "        ave_loss = loss_accum / i_step\n",
    "        val_f1 = compute_f1(model, val_loader)\n",
    "\n",
    "        loss_history.append(float(ave_loss))\n",
    "        val_history.append(val_f1)\n",
    "\n",
    "        print(\"Average loss: %f, Val f1: %f\" % (ave_loss, val_f1))\n",
    "\n",
    "    return loss_history, val_history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 1.480623, Val f1: 0.732673\n",
      "Average loss: 1.476205, Val f1: 0.693878\n",
      "Average loss: 1.478031, Val f1: 0.732673\n",
      "Average loss: 1.468294, Val f1: 0.732673\n",
      "Average loss: 1.482786, Val f1: 0.732673\n",
      "Average loss: 1.469846, Val f1: 0.757282\n",
      "Average loss: 1.452557, Val f1: 0.666667\n",
      "Average loss: 1.494706, Val f1: 0.745098\n",
      "Average loss: 1.472408, Val f1: 0.720000\n",
      "Average loss: 1.452492, Val f1: 0.720000\n",
      "Average loss: 1.417670, Val f1: 0.666667\n",
      "Average loss: 1.499693, Val f1: 0.757282\n",
      "Average loss: 1.463369, Val f1: 0.732673\n",
      "Average loss: 1.450621, Val f1: 0.825688\n",
      "Average loss: 1.494826, Val f1: 0.720000\n",
      "Average loss: 1.479028, Val f1: 0.720000\n",
      "Average loss: 1.497553, Val f1: 0.693878\n",
      "Average loss: 1.464062, Val f1: 0.732673\n",
      "Average loss: 1.463396, Val f1: 0.666667\n",
      "Average loss: 1.464835, Val f1: 0.680412\n",
      "Average loss: 1.490364, Val f1: 0.707071\n",
      "Average loss: 1.466642, Val f1: 0.680412\n",
      "Average loss: 1.468490, Val f1: 0.652632\n",
      "Average loss: 1.492148, Val f1: 0.745098\n",
      "Average loss: 1.481742, Val f1: 0.745098\n",
      "Average loss: 1.461680, Val f1: 0.757282\n",
      "Average loss: 1.455720, Val f1: 0.707071\n",
      "Average loss: 1.475247, Val f1: 0.757282\n",
      "Average loss: 1.480201, Val f1: 0.720000\n",
      "Average loss: 1.481216, Val f1: 0.757282\n",
      "Average loss: 1.480810, Val f1: 0.745098\n",
      "Average loss: 1.475332, Val f1: 0.757282\n",
      "Average loss: 1.474852, Val f1: 0.720000\n",
      "Average loss: 1.465521, Val f1: 0.707071\n",
      "Average loss: 1.471810, Val f1: 0.745098\n",
      "Average loss: 1.472718, Val f1: 0.732673\n",
      "Average loss: 1.467551, Val f1: 0.769231\n",
      "Average loss: 1.513039, Val f1: 0.666667\n",
      "Average loss: 1.476799, Val f1: 0.638298\n",
      "Average loss: 1.462137, Val f1: 0.638298\n",
      "Average loss: 1.488841, Val f1: 0.707071\n",
      "Average loss: 1.462859, Val f1: 0.707071\n",
      "Average loss: 1.473609, Val f1: 0.745098\n",
      "Average loss: 1.467133, Val f1: 0.680412\n",
      "Average loss: 1.458566, Val f1: 0.680412\n",
      "Average loss: 1.428742, Val f1: 0.652632\n",
      "Average loss: 1.473548, Val f1: 0.732673\n",
      "Average loss: 1.428859, Val f1: 0.707071\n",
      "Average loss: 1.513214, Val f1: 0.652632\n",
      "Average loss: 1.445944, Val f1: 0.638298\n",
      "Average loss: 1.495821, Val f1: 0.680412\n",
      "Average loss: 1.450339, Val f1: 0.757282\n",
      "Average loss: 1.484399, Val f1: 0.792453\n",
      "Average loss: 1.489641, Val f1: 0.623656\n",
      "Average loss: 1.446783, Val f1: 0.666667\n",
      "Average loss: 1.454961, Val f1: 0.780952\n",
      "Average loss: 1.438428, Val f1: 0.707071\n",
      "Average loss: 1.486271, Val f1: 0.666667\n",
      "Average loss: 1.481065, Val f1: 0.680412\n",
      "Average loss: 1.472423, Val f1: 0.732673\n",
      "Average loss: 1.447622, Val f1: 0.680412\n",
      "Average loss: 1.477808, Val f1: 0.757282\n",
      "Average loss: 1.459142, Val f1: 0.707071\n",
      "Average loss: 1.464468, Val f1: 0.666667\n",
      "Average loss: 1.478889, Val f1: 0.745098\n",
      "Average loss: 1.479034, Val f1: 0.757282\n",
      "Average loss: 1.470395, Val f1: 0.693878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/homry/OSLL/leti_hackaton22/venv/lib/python3.8/site-packages/facenet_pytorch/models/utils/detect_face.py:250: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if method is \"Min\":\n",
      "/home/homry/OSLL/leti_hackaton22/venv/lib/python3.8/site-packages/facenet_pytorch/models/utils/detect_face.py:250: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if method is \"Min\":\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam(parameters)\n\u001B[1;32m      9\u001B[0m loss \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mTripletMarginLoss(margin\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.0\u001B[39m, p\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m---> 10\u001B[0m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m compute_f1_test(resnet, test_loader)\n",
      "Cell \u001B[0;32mIn[8], line 7\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, train_loader, val_loader, loss, optimizer, num_epochs)\u001B[0m\n\u001B[1;32m      5\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain() \u001B[38;5;66;03m# Enter train mode\u001B[39;00m\n\u001B[1;32m      6\u001B[0m loss_accum \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m----> 7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i_step, (a, p, n) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader):\n\u001B[1;32m      8\u001B[0m     a_gpu \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor(a)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m      9\u001B[0m     p_gpu \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor(p)\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m~/OSLL/leti_hackaton22/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/OSLL/leti_hackaton22/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/OSLL/leti_hackaton22/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/OSLL/leti_hackaton22/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[0;32mIn[3], line 159\u001B[0m, in \u001B[0;36mLFW_Train.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, index):\n\u001B[1;32m    158\u001B[0m     img_inp \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mopen(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata[index][\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m--> 159\u001B[0m     anchor \u001B[38;5;241m=\u001B[39m \u001B[43mmtcnn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg_inp\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m    160\u001B[0m     anchor \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mreshape(anchor, (anchor\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], anchor\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m], \u001B[38;5;241m3\u001B[39m))\n\u001B[1;32m    162\u001B[0m     anchor \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mcvtColor(anchor, cv2\u001B[38;5;241m.\u001B[39mCOLOR_BGR2GRAY)\n",
      "File \u001B[0;32m~/OSLL/leti_hackaton22/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/OSLL/leti_hackaton22/venv/lib/python3.8/site-packages/facenet_pytorch/models/mtcnn.py:258\u001B[0m, in \u001B[0;36mMTCNN.forward\u001B[0;34m(self, img, save_path, return_prob)\u001B[0m\n\u001B[1;32m    227\u001B[0m \u001B[38;5;124;03m\"\"\"Run MTCNN face detection on a PIL image or numpy array. This method performs both\u001B[39;00m\n\u001B[1;32m    228\u001B[0m \u001B[38;5;124;03mdetection and extraction of faces, returning tensors representing detected faces rather\u001B[39;00m\n\u001B[1;32m    229\u001B[0m \u001B[38;5;124;03mthan the bounding boxes. To access bounding boxes, see the MTCNN.detect() method below.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    254\u001B[0m \u001B[38;5;124;03m>>> face_tensor, prob = mtcnn(img, save_path='face.png', return_prob=True)\u001B[39;00m\n\u001B[1;32m    255\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    257\u001B[0m \u001B[38;5;66;03m# Detect faces\u001B[39;00m\n\u001B[0;32m--> 258\u001B[0m batch_boxes, batch_probs, batch_points \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlandmarks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    259\u001B[0m \u001B[38;5;66;03m# Select faces\u001B[39;00m\n\u001B[1;32m    260\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkeep_all:\n",
      "File \u001B[0;32m~/OSLL/leti_hackaton22/venv/lib/python3.8/site-packages/facenet_pytorch/models/mtcnn.py:313\u001B[0m, in \u001B[0;36mMTCNN.detect\u001B[0;34m(self, img, landmarks)\u001B[0m\n\u001B[1;32m    273\u001B[0m \u001B[38;5;124;03m\"\"\"Detect all faces in PIL image and return bounding boxes and optional facial landmarks.\u001B[39;00m\n\u001B[1;32m    274\u001B[0m \n\u001B[1;32m    275\u001B[0m \u001B[38;5;124;03mThis method is used by the forward method and is also useful for face detection tasks\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;124;03m>>> img_draw.save('annotated_faces.png')\u001B[39;00m\n\u001B[1;32m    310\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    312\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 313\u001B[0m     batch_boxes, batch_points \u001B[38;5;241m=\u001B[39m \u001B[43mdetect_face\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    314\u001B[0m \u001B[43m        \u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmin_face_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    315\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43monet\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    316\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mthresholds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfactor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    317\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\n\u001B[1;32m    318\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    320\u001B[0m boxes, probs, points \u001B[38;5;241m=\u001B[39m [], [], []\n\u001B[1;32m    321\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m box, point \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(batch_boxes, batch_points):\n",
      "File \u001B[0;32m~/OSLL/leti_hackaton22/venv/lib/python3.8/site-packages/facenet_pytorch/models/utils/detect_face.py:72\u001B[0m, in \u001B[0;36mdetect_face\u001B[0;34m(imgs, minsize, pnet, rnet, onet, threshold, factor, device)\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m scale \u001B[38;5;129;01min\u001B[39;00m scales:\n\u001B[1;32m     71\u001B[0m     im_data \u001B[38;5;241m=\u001B[39m imresample(imgs, (\u001B[38;5;28mint\u001B[39m(h \u001B[38;5;241m*\u001B[39m scale \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m), \u001B[38;5;28mint\u001B[39m(w \u001B[38;5;241m*\u001B[39m scale \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)))\n\u001B[0;32m---> 72\u001B[0m     im_data \u001B[38;5;241m=\u001B[39m (\u001B[43mim_data\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m127.5\u001B[39;49m) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m0.0078125\u001B[39m\n\u001B[1;32m     73\u001B[0m     reg, probs \u001B[38;5;241m=\u001B[39m pnet(im_data)\n\u001B[1;32m     75\u001B[0m     boxes_scale, image_inds_scale \u001B[38;5;241m=\u001B[39m generateBoundingBox(reg, probs[:, \u001B[38;5;241m1\u001B[39m], scale, threshold[\u001B[38;5;241m0\u001B[39m])\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "params_old = list([param for name, param in resnet.named_parameters() if not name.startswith('logits.')])\n",
    "params_new = list([param for name, param in resnet.named_parameters() if name.startswith('logits.')])\n",
    "\n",
    "parameters = [\n",
    "    {'params': params_old, 'lr': 0.0001, 'momentum': 0.9},\n",
    "    {'params': params_new, 'lr': 0.001, 'momentum': 0.9}\n",
    "]\n",
    "optimizer = optim.Adam(parameters)\n",
    "loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "train_model(resnet, train_loader, val_loader, loss, optimizer, 100)\n",
    "compute_f1_test(resnet, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
